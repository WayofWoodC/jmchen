{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([428, 60, 4121, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 0])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "readpath='/data/disk3/DataBase_stocks/AllSample/'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU 不可用，将在 CPU 上运行\")\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
    "torch.cuda.empty_cache()  # 释放显存\n",
    "\n",
    "# 定义标准化函数\n",
    "def normalize_row(row):\n",
    "    data = row.dropna()\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    row[row.notna()] = (row[row.notna()] - mean) / std\n",
    "    return row\n",
    "df=pd.read_csv(readpath+'adjopen.csv',index_col=0)\n",
    "df.drop([x for x in df.columns if x[-2:]=='BJ'],axis=1,inplace=True)\n",
    "dfopen = df.apply(normalize_row, axis=1)\n",
    "df=pd.read_csv(readpath+'adjclose.csv',index_col=0)\n",
    "df.drop([x for x in df.columns if x[-2:]=='BJ'],axis=1,inplace=True)\n",
    "dfclose = df.apply(normalize_row, axis=1)\n",
    "df=pd.read_csv(readpath+'adjhigh.csv',index_col=0)\n",
    "df.drop([x for x in df.columns if x[-2:]=='BJ'],axis=1,inplace=True)\n",
    "dfhigh = df.apply(normalize_row, axis=1)\n",
    "df=pd.read_csv(readpath+'adjlow.csv',index_col=0)\n",
    "df.drop([x for x in df.columns if x[-2:]=='BJ'],axis=1,inplace=True)\n",
    "dflow = df.apply(normalize_row, axis=1)\n",
    "df=pd.read_csv(readpath+'volume.csv',index_col=0)\n",
    "df.drop([x for x in df.columns if x[-2:]=='BJ'],axis=1,inplace=True)\n",
    "dfvolume = df.apply(normalize_row, axis=1)\n",
    "df=pd.read_csv(readpath+'vwap_adj.csv',index_col=0)\n",
    "df.drop([x for x in df.columns if x[-2:]=='BJ'],axis=1,inplace=True)\n",
    "dfvwap = df.apply(normalize_row, axis=1)\n",
    "dfs=[dfclose,dfopen,dfhigh,dflow,dfvwap,dfvolume]\n",
    "dff=[]\n",
    "for df in dfs:  #删除测试集时间段内未上市的\n",
    "    df_filled = df[(df.index < 20210101) & (df.index>=20190101)]\n",
    "    df_filled=df_filled.dropna(axis=1,how='all')\n",
    "    df_filled = df_filled.fillna(method='bfill')\n",
    "    dff.append(df_filled)\n",
    "\n",
    "dfm=pd.concat(dff,axis=0,join='inner')\n",
    "columns_to_drop = []\n",
    "for column in dfm.columns:\n",
    "    # 将每列转换为布尔值，True表示空值，False表示非空值\n",
    "    is_null = dfm[column].isnull()\n",
    "    # 使用rolling函数计算连续空值的长度\n",
    "    consecutive_nulls = is_null.rolling(5, min_periods=1).sum()\n",
    "    # 检查是否存在连续空值长度大于等于5的情况\n",
    "    if any(consecutive_nulls >= 5):\n",
    "        columns_to_drop.append(column)\n",
    "for i in range(len(dff)):\n",
    "    dff[i] = dff[i].drop(columns=columns_to_drop)\n",
    "#再对齐一下列，不知道为什么concat后还是没对齐：\n",
    "dfmm=pd.concat(dff,axis=0,join='inner')\n",
    "for i in range(len(dff)):\n",
    "    dff[i] = dff[i][dff[i].columns.intersection(dfmm.columns)]\n",
    "time_length =487\n",
    "stock_code_length = 4121\n",
    "T=60\n",
    "# 创建一个空的三维数组，用于存放合并后的数据\n",
    "X_tensor = np.zeros((time_length, stock_code_length, 6))\n",
    "\n",
    "# 合并6个DataFrame的数据\n",
    "for i, df in enumerate(dff):\n",
    "    # 将DataFrame的值复制到对应的tensor切片中\n",
    "    X_tensor[:, :, i] = df.values\n",
    "X_tensor=torch.Tensor(X_tensor)\n",
    "total_samples = 428  # 指定总样本数量\n",
    "time_steps_per_sample = 60  # 指定每个样本的时间步数\n",
    "num_stocks = 4121  # 股票数量\n",
    "num_features = 6  # 特征数量\n",
    "\n",
    "# 初始化新的X_train\n",
    "X_train = torch.zeros((total_samples, time_steps_per_sample, num_stocks, num_features))\n",
    "\n",
    "# 将数据拆分成样本\n",
    "\n",
    "for i in range(total_samples):\n",
    "    end_idx = i + time_steps_per_sample\n",
    "    X_train[i] = X_tensor[i:end_idx]\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 4121])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twap=pd.read_csv('twap_all.csv',index_col=[0])\n",
    "adj=pd.read_csv('adjfactor.csv',index_col=[0])\n",
    "twap.columns=[x[:6] for x in twap.columns]\n",
    "adj.columns=[x[:6] for x in adj.columns]\n",
    "open=twap*adj\n",
    "# open.columns=open.columns.astype(int)\n",
    "ret=(open-open.shift(1))/open.shift(1)\n",
    "ret=ret.shift(-1)\n",
    "ret= ret.fillna(method='bfill')\n",
    "ret=ret[(ret.index < 20210105) & (ret.index>=20190101)]\n",
    "columns=[x.split('.')[0] for x in dff[0].columns]\n",
    "ret= ret[ret.columns.intersection(columns)]\n",
    "ret=ret.iloc[60:,]\n",
    "ret\n",
    "array = ret.to_numpy()  # 将DataFrame转换为NumPy数组\n",
    "Y_train = np.reshape(array, ret.shape)  # 将数组reshape为张量\n",
    "Y_train=torch.Tensor(Y_train)\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_factors, num_stocks):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.num_stocks = num_stocks\n",
    "        self.num_factors=num_factors\n",
    "        \n",
    "        # 定义LSTM层\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # 定义全连接层\n",
    "        self.fc = nn.Linear(hidden_size, num_factors)  #h\n",
    "        \n",
    "        # 定义批标准化层\n",
    "        self.bn = nn.BatchNorm1d(num_factors)  #z\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, T, _, num_features = x.size()\n",
    "        \n",
    "        # 将num_stocks维度移到batch_size之后\n",
    "        x = x.view(-1, T, num_features)\n",
    "        \n",
    "        # LSTM层\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # 最后一个时间步的输出\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # 全连接层\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        \n",
    "        # 批标准化层\n",
    "        factor_output = self.bn(fc_out)\n",
    "        \n",
    "        # 将结果恢复成(batch_size, num_stocks, num_factors)形状\n",
    "        factor_output = factor_output.view(batch_size, self.num_stocks, num_factors)\n",
    "        \n",
    "        return factor_output\n",
    "# 数据准备：假设有num_stocks只股票，每只股票有num_features个特征\n",
    "num_stocks = 4121\n",
    "num_features = 6\n",
    "T = 60  # 假设时间步为30\n",
    "num_epochs = 8\n",
    "batch_size = 4\n",
    "\n",
    "# 创建数据，X_train的形状应为(训练集总长, T, num_stocks, num_features)，y_train的形状应为(训练集总长, num_stocks)\n",
    "X_train\n",
    "Y_train \n",
    "\n",
    "# 模型构建\n",
    "\n",
    "# 创建模型实例\n",
    "input_size = num_features\n",
    "hidden_size = 64\n",
    "num_factors = 1  #60个等权输出？\n",
    "\n",
    "model = MyModel(input_size, hidden_size, num_factors, num_stocks)\n",
    "\n",
    "\n",
    "# model = DataParallel(model)\n",
    "model.to(device)  # 将模型移动到GPU\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    torch.cuda.empty_cache() \n",
    "    for i in range(0, X_train.size(0)-4, batch_size):\n",
    "        inputs = X_train[i:i+batch_size].to(device)\n",
    "        labels = Y_train[i:i+batch_size].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 计算损失函数\n",
    "        loss = 0\n",
    "        for j in range(num_stocks):\n",
    "            # 对于每只股票，计算均方误差损失\n",
    "            mse_loss = F.mse_loss(outputs[:, j, :], labels[:, j],reduction='mean')\n",
    "            loss += mse_loss  # 或者 mae_loss，根据需求选择\n",
    "            \n",
    "        print(epoch,i,loss.item()/num_stocks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_params.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 4121)\n"
     ]
    }
   ],
   "source": [
    "dfftest=[]\n",
    "for df in dfs:  #删除测试集时间段内未上市的\n",
    "    df_filled = df[(df.index < 20211231) & (df.index>=20210101)]\n",
    "    df_filled=df_filled.dropna(axis=1,how='all')\n",
    "    df_filled = df_filled.fillna(method='bfill')\n",
    "    dfftest.append(df_filled)\n",
    "for i in range(len(dff)):\n",
    "    dfftest[i] = dfftest[i][dfftest[i].columns.intersection(dfmm.columns)]\n",
    "print(dfftest[0].shape)\n",
    "time_length =242\n",
    "stock_code_length = 4121\n",
    "T=60\n",
    "# 创建一个空的三维数组，用于存放合并后的数据\n",
    "X_tensortest = np.zeros((time_length, stock_code_length, 6))\n",
    "\n",
    "# 合并6个DataFrame的数据\n",
    "for i, df in enumerate(dfftest):\n",
    "    # 将DataFrame的值复制到对应的tensor切片中\n",
    "    X_tensortest[:, :, i] = df.values\n",
    "X_tensortest=torch.Tensor(X_tensortest)\n",
    "total_samples = 183  # 指定总样本数量\n",
    "time_steps_per_sample = 60  # 指定每个样本的时间步数\n",
    "num_stocks = 4121  # 股票数量\n",
    "num_features = 6  # 特征数量\n",
    "\n",
    "# 初始化新的X_train\n",
    "X_test = torch.zeros((total_samples, time_steps_per_sample, num_stocks, num_features))\n",
    "\n",
    "# 将数据拆分成样本\n",
    "\n",
    "for i in range(total_samples):\n",
    "    end_idx = i + time_steps_per_sample\n",
    "    X_test[i] = X_tensortest[i:end_idx]\n",
    "\n",
    "close=pd.read_csv('adjclose.csv',index_col=0)\n",
    "close.drop([x for x in close.columns if x[-2:]=='BJ'],axis=1,inplace=True)\n",
    "ret=(close-close.shift(1))/close.shift(1)\n",
    "ret= ret.fillna(method='bfill')\n",
    "ret=ret[(ret.index <= 20211231) & (ret.index>=20210101)]\n",
    "ret= ret[ret.columns.intersection(dff[0].columns)]\n",
    "ret=ret.iloc[T:,]\n",
    "array = ret.to_numpy()  # 将DataFrame转换为NumPy数组\n",
    "Y_test = np.reshape(array, ret.shape)  # 将数组reshape为张量\n",
    "Y_test=torch.Tensor(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([183, 60, 4121, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_factors, num_stocks):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.num_stocks = num_stocks\n",
    "        self.num_factors= num_factors\n",
    "        \n",
    "        # 定义LSTM层\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size,num_layers=2, batch_first=True)\n",
    "        \n",
    "        # 定义全连接层\n",
    "        self.fc = nn.Linear(hidden_size, num_factors)  #h\n",
    "        \n",
    "        # 定义批标准化层\n",
    "        self.bn = nn.BatchNorm1d(num_factors)  #z\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, T, _, num_features = x.size()\n",
    "        \n",
    "        # 将num_stocks维度移到batch_size之后\n",
    "        x = x.view(-1, T, num_features)\n",
    "        \n",
    "        # LSTM层\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # 最后一个时间步的输出\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # 全连接层\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        \n",
    "        # 批标准化层\n",
    "        factor_output = self.bn(fc_out)\n",
    "        \n",
    "        # 将结果恢复成(batch_size, num_stocks, num_factors)形状\n",
    "        factor_output = factor_output.view(batch_size, self.num_stocks, num_factors)\n",
    "        \n",
    "        return factor_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 225687875584 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/data/disk4/output_stocks/jmchen/factors/ML/LSTM.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.8.107/data/disk4/output_stocks/jmchen/factors/ML/LSTM.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m label_sample \u001b[39m=\u001b[39m Y_test\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.8.107/data/disk4/output_stocks/jmchen/factors/ML/LSTM.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# 使用模型进行推理并获取单个样本的预测输出（c值）\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.8.107/data/disk4/output_stocks/jmchen/factors/ML/LSTM.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m factors \u001b[39m=\u001b[39m model(input_sample)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.8.107/data/disk4/output_stocks/jmchen/factors/ML/LSTM.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m c_value\u001b[39m=\u001b[39mfactors\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.8.107/data/disk4/output_stocks/jmchen/factors/ML/LSTM.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(c_value\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/data/disk4/output_stocks/jmchen/factors/ML/LSTM.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.8.107/data/disk4/output_stocks/jmchen/factors/ML/LSTM.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, T, num_features)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.8.107/data/disk4/output_stocks/jmchen/factors/ML/LSTM.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# LSTM层\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.8.107/data/disk4/output_stocks/jmchen/factors/ML/LSTM.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m lstm_out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.8.107/data/disk4/output_stocks/jmchen/factors/ML/LSTM.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# 最后一个时间步的输出\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.8.107/data/disk4/output_stocks/jmchen/factors/ML/LSTM.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m lstm_out \u001b[39m=\u001b[39m lstm_out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 225687875584 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "torch.cuda.empty_cache()\n",
    "#input_size, hidden_size, num_factors, num_stocks\n",
    "model = MyModel(6,64,60,4121)\n",
    "# 加载保存的模型参数\n",
    "model.load_state_dict(torch.load('model_params_long2.pth'))\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "ic_values = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_sample = X_test  # 获取单个样本并添加批次维度\n",
    "    label_sample = Y_test\n",
    "    # 使用模型进行推理并获取单个样本的预测输出（c值）\n",
    "    factors = model(input_sample)\n",
    "    c_value=factors.mean(dim=2)\n",
    "    print(c_value.shape)\n",
    "    print(Y_test.shape)\n",
    "    corr1=[]\n",
    "    corr2=[]\n",
    "    for i in range(c_value.shape[0]): \n",
    "        # 创建遮罩以识别c_value和Y_test中的非NaN值\n",
    "        mask_c = ~torch.isnan(c_value[i])\n",
    "        mask_y = ~torch.isnan(Y_test[i])\n",
    "        \n",
    "        # 组合这些遮罩以获取c_value和Y_test的共同遮罩\n",
    "        mask = mask_c & mask_y\n",
    "\n",
    "        # 将遮罩应用于张量\n",
    "        c_value_filtered = c_value[i][mask]\n",
    "        Y_test_filtered = Y_test[i][mask]\n",
    "\n",
    "    \n",
    "        # 计算每个输入的均值\n",
    "        mean_c = torch.mean(c_value_filtered)\n",
    "        mean_y = torch.mean(Y_test_filtered)\n",
    "\n",
    "        # 计算皮尔逊相关系数的分子和分母\n",
    "        numerator = torch.sum((c_value_filtered - mean_c) * (Y_test_filtered - mean_y))\n",
    "        denominator_c = torch.sqrt(torch.sum((c_value_filtered - mean_c)**2))\n",
    "        denominator_y = torch.sqrt(torch.sum((Y_test_filtered - mean_y)**2))\n",
    "\n",
    "        # 计算皮尔逊相关系数\n",
    "        pearson_corr = numerator / (denominator_c * denominator_y)\n",
    "        corr1.append(pearson_corr.item())\n",
    "        # 计算秩次差值\n",
    "        rank_X = torch.argsort(c_value[i].reshape(-1))\n",
    "        rank_Y = torch.argsort(Y_test[i].reshape(-1))\n",
    "        differences = rank_X - rank_Y\n",
    "        # 计算斯皮尔曼秩相关系数\n",
    "        n = len(c_value[i])\n",
    "        spearman_corr = 1 - (6 * torch.sum(differences**2)) / (n * (n**2 - 1))\n",
    "        corr2.append(spearman_corr)\n",
    "    corr1=np.mean(corr1)\n",
    "    corr2=np.mean(corr2)\n",
    "    print('Pearson:',corr1)\n",
    "    print('Spearman:',corr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00042065963876592013\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "# 将测试集数据（X_test）和标签（Y_test）移动到GPU（如果模型在GPU上）\n",
    "# 存储每个样本的IC值\n",
    "ic_values = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(X_test.size(0)):\n",
    "        input_sample = X_test[i].unsqueeze(0)  # 获取单个样本并添加批次维度\n",
    "        label_sample = Y_test[i]\n",
    "        # 使用模型进行推理并获取单个样本的预测输出（c值）\n",
    "        c_value = model(input_sample)\n",
    "\n",
    "        # 将预测结果与实际标签转换为NumPy数组\n",
    "        c_value = c_value.squeeze().cpu().numpy()\n",
    "        label_sample = label_sample.cpu().numpy()\n",
    "        c_value=np.nan_to_num(c_value, nan=0)\n",
    "        label_sample=np.nan_to_num(label_sample, nan=0)\n",
    "        # 计算单个样本的IC值并添加到列表中\n",
    "        ic_sample, _ = pearsonr(label_sample, c_value)\n",
    "        ic_values.append(ic_sample)\n",
    "print(np.mean(ic_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    ###input_size:输入的特征维度  hidden_size:隐藏层神经元个数\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        # 定义LSTM层\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # 定义全连接层\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # 定义批标准化层\n",
    "        self.bn = nn.BatchNorm1d(output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM层\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # 最后一个时间步的输出\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # 全连接层\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        \n",
    "        # 批标准化层\n",
    "        factor_output = self.bn(fc_out)\n",
    "        \n",
    "        return factor_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义损失函数\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # 计算损失函数，可以根据实际需求进行调整\n",
    "    loss = torch.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    # 计算标准化因子相关性矩阵的L2范数\n",
    "    factor_corr_matrix = torch.matmul(torch.transpose(y_pred, 0, 1), y_pred)\n",
    "    l2_penalty = torch.norm(factor_corr_matrix, p='fro')\n",
    "    \n",
    "    # 添加相关性惩罚项\n",
    "    loss += lambda_penalty * l2_penalty\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# 定义相关性惩罚项的权重\n",
    "lambda_penalty = 0.001  # 根据实际需求进行调整\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型实例\n",
    "input_size = 6  # 输入特征的数量\n",
    "hidden_size = 64  # LSTM隐藏层的单元数\n",
    "output_size = 1  # 多元因子的数量,先只预测收益率\n",
    "\n",
    "model = MyModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 定义训练数据和标签的Tensor\n",
    "X_train = torch.Tensor()  # 训练数据，形状为 (样本数量, T, num_features)(,60,)\n",
    "y_train = torch.Tensor()  # 标签，形状为 (样本数量, output_size)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 100  # 根据实际需求进行调整\n",
    "batch_size = 32  # 根据实际需求进行调整\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('epoch:',epoch)\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        inputs = X_train[i:i+batch_size]\n",
    "        labels = y_train[i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = custom_loss(labels, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_factors, num_stocks):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.num_stocks = num_stocks\n",
    "        \n",
    "        # 定义LSTM层\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # 定义全连接层\n",
    "        self.fc = nn.Linear(hidden_size, num_factors)\n",
    "        \n",
    "        # 定义批标准化层\n",
    "        self.bn = nn.BatchNorm1d(num_factors)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, T, _, num_features = x.size()\n",
    "        \n",
    "        # 将num_stocks维度移到batch_size之后\n",
    "        x = x.view(-1, T, num_features)\n",
    "        \n",
    "        # LSTM层\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # 最后一个时间步的输出\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # 全连接层\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        \n",
    "        # 批标准化层\n",
    "        factor_output = self.bn(fc_out)\n",
    "        \n",
    "        # 将结果恢复成(batch_size, num_stocks, num_factors)形状\n",
    "        factor_output = factor_output.view(batch_size, self.num_stocks, -1)\n",
    "        \n",
    "        return factor_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 数据准备：假设有num_stocks只股票，每只股票有num_features个特征\n",
    "num_stocks = 5000\n",
    "num_features = 6\n",
    "T = 60  # 假设时间步为30\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# 创建数据，X_train的形状应为(训练集总长, T, num_stocks, num_features)，y_train的形状应为(训练集总长, num_stocks)\n",
    "X_train = torch.Tensor(...)  # 根据实际数据填充\n",
    "y_train = torch.Tensor(...)  # 根据实际数据填充\n",
    "\n",
    "# 模型构建\n",
    "\n",
    "# 创建模型实例\n",
    "input_size = num_features\n",
    "hidden_size = 64\n",
    "num_factors = 10\n",
    "\n",
    "model = MyModel(input_size, hidden_size, num_factors, num_stocks)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        inputs = X_train[i:i+batch_size]\n",
    "        labels = y_train[i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 计算损失函数\n",
    "        loss = 0\n",
    "        for j in range(num_stocks):\n",
    "            loss += custom_loss(labels[:, j], outputs[:, j, :])\n",
    "        \n",
    "        print(epoch,loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # 释放显存\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
