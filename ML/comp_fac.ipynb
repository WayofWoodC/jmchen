{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU 不可用，将在 CPU 上运行\")\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
    "torch.cuda.empty_cache()  # 释放显存\n",
    "\n",
    "# 定义标准化函数\n",
    "def normalize_row(row):\n",
    "    data = row.dropna()\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    row[row.notna()] = (row[row.notna()] - mean) / std\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#因子正交\n",
    "class MyModelZJ(nn.Module):\n",
    "    def __init__(self, input_size,num_factors, num_stocks):\n",
    "        super(MyModelZJ, self).__init__()\n",
    "        self.num_factors = num_factors\n",
    "        self.num_stocks = num_stocks\n",
    "               \n",
    "        # 定义全连接层\n",
    "        self.fc = nn.Linear(input_size, num_factors)  #\n",
    "        \n",
    "        # 定义批标准化层\n",
    "        self.bn = nn.BatchNorm1d(num_factors)  #\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, num_features = x.size()\n",
    "        # print(x.shape)\n",
    "        # 将num_stocks维度移到batch_size之后\n",
    "        x = x.view(-1, num_features)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # 全连接层\n",
    "        fc_out = self.fc(x)\n",
    "        # print('全连接层',fc_out.shape)\n",
    "        # 批标准化层\n",
    "        fc_out=fc_out.view(batch_size*self.num_stocks,self.num_factors)\n",
    "        factor_output = self.bn(fc_out)\n",
    "        # print('标准化层',factor_output.shape)\n",
    "        # 将结果恢复成(batch_size, num_stocks, num_factors)形状\n",
    "        factor_output = factor_output.view(batch_size, self.num_stocks, self.num_factors)\n",
    "        \n",
    "        # 因子等权求和\n",
    "        # c = factor_output.mean(dim=2)\n",
    "        # print(c.shape)\n",
    "        return factor_output\n",
    "# model = MyModelZJ(1200,300,1000)\n",
    "# x=torch.rand(4,1000,1200)\n",
    "# y=model(x)\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 60, 400, 6])\n",
      "torch.Size([1600, 60, 6])\n",
      "lstmout torch.Size([1600, 60, 64])\n",
      "最后时间步 torch.Size([1600, 10, 64])\n",
      "全连接层 torch.Size([1600, 10, 5])\n",
      "标准化层 torch.Size([16000, 5])\n",
      "torch.Size([4, 400, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#多因子预测\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,num_layers,dropout, num_factors, num_stocks,preday):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.num_factors = num_factors\n",
    "        self.num_stocks = num_stocks\n",
    "        self.preday=preday\n",
    "        \n",
    "        # 定义LSTM层\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers=2,batch_first=True)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,dropout=dropout,batch_first=True)\n",
    "        \n",
    "        # 定义全连接层\n",
    "        self.fc = nn.Linear(hidden_size, num_factors)  #h\n",
    "        \n",
    "        # 定义批标准化层\n",
    "        self.bn = nn.BatchNorm1d(num_factors)  #z\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, T, _, num_features = x.size()\n",
    "        print(x.shape)\n",
    "        # 将num_stocks维度移到batch_size之后\n",
    "        x = x.view(-1, T, num_features)\n",
    "        print(x.shape)\n",
    "        # LSTM层\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        print('lstmout',lstm_out.shape)\n",
    "        # 最后10个时间步的输出\n",
    "        lstm_out = lstm_out[:, -self.preday:, :]\n",
    "        print('最后时间步',lstm_out.shape)\n",
    "        # 全连接层\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        print('全连接层',fc_out.shape)\n",
    "        # 批标准化层\n",
    "        fc_out=fc_out.view(batch_size*self.num_stocks*self.preday,self.num_factors)\n",
    "        factor_output = self.bn(fc_out)\n",
    "        print('标准化层',factor_output.shape)\n",
    "        # 将结果恢复成(batch_size, num_stocks, num_factors)形状\n",
    "        factor_output = factor_output.view(batch_size, self.num_stocks,self.preday, self.num_factors)\n",
    "        \n",
    "        # 计算c，这里直接求因子平均\n",
    "        c = factor_output.mean(dim=3)\n",
    "        print(c.shape)\n",
    "        return factor_output\n",
    "model = MyModel(6, 64, 1,0,5, 400,10)\n",
    "x=torch.rand(4,60,400,6)\n",
    "y=model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=os.listdir('/data/disk3/output_stocks/ytcheng/ks_sig_pool_daily')\n",
    "files.sort()\n",
    "dfs=[]\n",
    "for file in files:\n",
    "    date=file.replace('-','')\n",
    "    date=int(date.split('.')[0])\n",
    "    if date<20200101:\n",
    "        continue\n",
    "    print(date)\n",
    "    df=pd.read_csv('/data/disk3/output_stocks/ytcheng/ks_sig_pool_daily/'+file,index_col=0)\n",
    "    # not_nan=df.notna().sum(axis=1)\n",
    "    # df=df[not_nan>0.8*len(df.columns)]\n",
    "    df.insert(0,'date',date)\n",
    "    dfs.append(df)\n",
    "dfs=pd.concat(dfs,axis=0,join='inner')\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\n",
      "1         243\n",
      "600775    243\n",
      "600757    243\n",
      "600758    243\n",
      "600759    243\n",
      "         ... \n",
      "300030    243\n",
      "300742    243\n",
      "300751    243\n",
      "300028    243\n",
      "300045    243\n",
      "Name: count, Length: 4088, dtype: int64\n",
      "20200102\n",
      "20200103\n",
      "20200106\n",
      "20200107\n",
      "20200108\n",
      "20200109\n",
      "20200110\n",
      "20200113\n",
      "20200114\n",
      "20200115\n",
      "20200116\n",
      "20200117\n",
      "20200120\n",
      "20200121\n",
      "20200122\n",
      "20200123\n",
      "20200203\n",
      "20200204\n",
      "20200205\n",
      "20200206\n",
      "20200207\n",
      "20200210\n",
      "20200211\n",
      "20200212\n",
      "20200213\n",
      "20200214\n",
      "20200217\n",
      "20200218\n",
      "20200219\n",
      "20200220\n",
      "20200221\n",
      "20200224\n",
      "20200225\n",
      "20200226\n",
      "20200227\n",
      "20200228\n",
      "20200302\n",
      "20200303\n",
      "20200304\n",
      "20200305\n",
      "20200306\n",
      "20200309\n",
      "20200310\n",
      "20200311\n",
      "20200312\n",
      "20200313\n",
      "20200316\n",
      "20200317\n",
      "20200318\n",
      "20200319\n",
      "20200320\n",
      "20200323\n",
      "20200324\n",
      "20200325\n",
      "20200326\n",
      "20200327\n",
      "20200330\n",
      "20200331\n",
      "20200401\n",
      "20200402\n",
      "20200403\n",
      "20200407\n",
      "20200408\n",
      "20200409\n",
      "20200410\n",
      "20200413\n",
      "20200414\n",
      "20200415\n",
      "20200416\n",
      "20200417\n",
      "20200420\n",
      "20200421\n",
      "20200422\n",
      "20200423\n",
      "20200424\n",
      "20200427\n",
      "20200428\n",
      "20200429\n",
      "20200430\n",
      "20200506\n",
      "20200507\n",
      "20200508\n",
      "20200511\n",
      "20200512\n",
      "20200513\n",
      "20200514\n",
      "20200515\n",
      "20200518\n",
      "20200519\n",
      "20200520\n",
      "20200521\n",
      "20200522\n",
      "20200525\n",
      "20200526\n",
      "20200527\n",
      "20200528\n",
      "20200529\n",
      "20200601\n",
      "20200602\n",
      "20200603\n",
      "20200604\n",
      "20200605\n",
      "20200608\n",
      "20200609\n",
      "20200610\n",
      "20200611\n",
      "20200612\n",
      "20200615\n",
      "20200616\n",
      "20200617\n",
      "20200618\n",
      "20200619\n",
      "20200622\n",
      "20200623\n",
      "20200624\n",
      "20200629\n",
      "20200630\n",
      "20200701\n",
      "20200702\n",
      "20200703\n",
      "20200706\n",
      "20200707\n",
      "20200708\n",
      "20200709\n",
      "20200710\n",
      "20200713\n",
      "20200714\n",
      "20200715\n",
      "20200716\n",
      "20200717\n",
      "20200720\n",
      "20200721\n",
      "20200722\n",
      "20200723\n",
      "20200724\n",
      "20200727\n",
      "20200728\n",
      "20200729\n",
      "20200730\n",
      "20200731\n",
      "20200803\n",
      "20200804\n",
      "20200805\n",
      "20200806\n",
      "20200807\n",
      "20200810\n",
      "20200811\n",
      "20200812\n",
      "20200813\n",
      "20200814\n",
      "20200817\n",
      "20200818\n",
      "20200819\n",
      "20200820\n",
      "20200821\n",
      "20200824\n",
      "20200825\n",
      "20200826\n",
      "20200827\n",
      "20200828\n",
      "20200831\n",
      "20200901\n",
      "20200902\n",
      "20200903\n",
      "20200904\n",
      "20200907\n",
      "20200908\n",
      "20200909\n",
      "20200910\n",
      "20200911\n",
      "20200914\n",
      "20200915\n",
      "20200916\n",
      "20200917\n",
      "20200918\n",
      "20200921\n",
      "20200922\n",
      "20200923\n",
      "20200924\n",
      "20200925\n",
      "20200928\n",
      "20200929\n",
      "20200930\n",
      "20201009\n",
      "20201012\n",
      "20201013\n",
      "20201014\n",
      "20201015\n",
      "20201016\n",
      "20201019\n",
      "20201020\n",
      "20201021\n",
      "20201022\n",
      "20201023\n",
      "20201026\n",
      "20201027\n",
      "20201028\n",
      "20201029\n",
      "20201030\n",
      "20201102\n",
      "20201103\n",
      "20201104\n",
      "20201105\n",
      "20201106\n",
      "20201109\n",
      "20201110\n",
      "20201111\n",
      "20201112\n",
      "20201113\n",
      "20201116\n",
      "20201117\n",
      "20201118\n",
      "20201119\n",
      "20201120\n",
      "20201123\n",
      "20201124\n",
      "20201125\n",
      "20201126\n",
      "20201127\n",
      "20201130\n",
      "20201201\n",
      "20201202\n",
      "20201203\n",
      "20201204\n",
      "20201207\n",
      "20201208\n",
      "20201209\n",
      "20201210\n",
      "20201211\n",
      "20201214\n",
      "20201215\n",
      "20201216\n",
      "20201217\n",
      "20201218\n",
      "20201221\n",
      "20201222\n",
      "20201223\n",
      "20201224\n",
      "20201225\n",
      "20201228\n",
      "20201229\n",
      "20201230\n",
      "20201231\n"
     ]
    }
   ],
   "source": [
    "df20=pd.read_feather('/data/disk4/output_stocks/jmchen/factors/ML/combine20.feather')\n",
    "index_counts = df20['index'].value_counts()\n",
    "index_counts=index_counts[index_counts==index_counts.max()]\n",
    "print(index_counts)\n",
    "dfx=df20[df20['index'].isin(index_counts.index)]\n",
    "dfx=dfx.fillna(0)\n",
    "ds=dfx.groupby('date')\n",
    "days=index_counts.max()\n",
    "num_stocks=4088\n",
    "num_features=4321\n",
    "X = torch.zeros((days, num_stocks, num_features))\n",
    "i=0\n",
    "for day,df in ds:\n",
    "    print(day)\n",
    "    df = df.drop(columns=['date'])\n",
    "    df=df.set_index('index')\n",
    "    x=torch.Tensor(df.values)\n",
    "    X[i]=x\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义标准化函数\n",
    "def normalize_row(row):\n",
    "    data = row.dropna()\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    row[row.notna()] = (row[row.notna()] - mean) / std\n",
    "    return row\n",
    "df=pd.read_csv('/data/disk3/DataBase_stocks/AllSample/adjopen.csv',index_col=0)\n",
    "df.drop([x for x in df.columns if x[-2:]=='BJ'],axis=1,inplace=True)\n",
    "dfopen = df.apply(normalize_row, axis=1)\n",
    "df=pd.read_csv('/data/disk3/DataBase_stocks/AllSample/adjclose.csv',index_col=0)\n",
    "df.drop([x for x in df.columns if x[-2:]=='BJ'],axis=1,inplace=True)\n",
    "dfclose = df.apply(normalize_row, axis=1)\n",
    "df=pd.read_csv('/data/disk3/DataBase_stocks/AllSample/adjhigh.csv',index_col=0)\n",
    "df.drop([x for x in df.columns if x[-2:]=='BJ'],axis=1,inplace=True)\n",
    "dfhigh = df.apply(normalize_row, axis=1)\n",
    "df=pd.read_csv('/data/disk3/DataBase_stocks/AllSample/adjlow.csv',index_col=0)\n",
    "df.drop([x for x in df.columns if x[-2:]=='BJ'],axis=1,inplace=True)\n",
    "dflow = df.apply(normalize_row, axis=1)\n",
    "df=pd.read_csv('/data/disk3/DataBase_stocks/AllSample/volume.csv',index_col=0)\n",
    "df.drop([x for x in df.columns if x[-2:]=='BJ'],axis=1,inplace=True)\n",
    "dfvolume = df.apply(normalize_row, axis=1)\n",
    "df=pd.read_csv('/data/disk3/DataBase_stocks/AllSample/vwap_adj.csv',index_col=0)\n",
    "df.drop([x for x in df.columns if x[-2:]=='BJ'],axis=1,inplace=True)\n",
    "dfvwap = df.apply(normalize_row, axis=1)\n",
    "dfs=[dfclose,dfopen,dfhigh,dflow,dfvwap,dfvolume]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "close=pd.read_csv('/data/disk3/DataBase_stocks/AllSample/adjclose.csv',index_col=0)\n",
    "close.drop([x for x in close.columns if x[-2:]=='BJ'],axis=1,inplace=True)\n",
    "close.columns=[int(x.split('.')[0]) for x in close.columns]\n",
    "ret=(close-close.shift(1))/close.shift(1)  #少个shift\n",
    "ret= ret.fillna(method='bfill')\n",
    "ret=ret.fillna(0)\n",
    "ret = ret.apply(normalize_row, axis=1) #标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([243, 4088])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret1=ret[(ret.index <=20210104) & (ret.index>=20200103)]  #后一天收益率为标签\n",
    "ret1= ret1[ret1.columns.intersection(index_counts.index)]\n",
    "Y=torch.Tensor(ret1.values)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=[]\n",
    "for df in dfs:  #删除训练集时间段内未上市的\n",
    "    df_filled = df[(df.index < 20200101) & (df.index>20100101)]\n",
    "    df_filled=df_filled.dropna(axis=1,how='all') #删空列\n",
    "    df_filled = df_filled.fillna(method='bfill')  #缺值回填\n",
    "    dff.append(df_filled)\n",
    "\n",
    "dfm=pd.concat(dff,axis=0,join='inner')\n",
    "columns_to_drop = []\n",
    "for column in dfm.columns: #若之后退市同样忽略\n",
    "    # 将每列转换为布尔值，True表示空值，False表示非空值\n",
    "    is_null = dfm[column].isnull()\n",
    "    # 使用rolling函数计算连续空值的长度 \n",
    "    consecutive_nulls = is_null.rolling(5, min_periods=1).sum()\n",
    "    # 检查是否存在连续空值长度大于等于5的情况\n",
    "    if any(consecutive_nulls >= 5):\n",
    "        columns_to_drop.append(column)\n",
    "for i in range(len(dff)):\n",
    "    dff[i] = dff[i].drop(columns=columns_to_drop)\n",
    "#再对齐一下列，不知道为什么concat后还是没对齐：\n",
    "dfmm=pd.concat(dff,axis=0,join='inner')\n",
    "for i in range(len(dff)):\n",
    "    dff[i] = dff[i][dff[i].columns.intersection(dfmm.columns)]\n",
    "\n",
    "dffval=[]\n",
    "for df in dfs:  #删除训练集时间段内未上市的\n",
    "    df_filled = df[(df.index < 20210101) & (df.index>20200101)]\n",
    "    df_filled=df_filled.dropna(axis=1,how='all') #删空列\n",
    "    df_filled = df_filled.fillna(method='bfill')  #缺值回填\n",
    "    dffval.append(df_filled)\n",
    "\n",
    "dfmval=pd.concat(dffval,axis=0,join='inner')\n",
    "# columns_to_drop = []\n",
    "# for column in dfm.columns: #若之后退市同样忽略\n",
    "#     # 将每列转换为布尔值，True表示空值，False表示非空值\n",
    "#     is_null = dfm[column].isnull()\n",
    "#     # 使用rolling函数计算连续空值的长度 \n",
    "#     consecutive_nulls = is_null.rolling(5, min_periods=1).sum()\n",
    "#     # 检查是否存在连续空值长度大于等于5的情况\n",
    "#     if any(consecutive_nulls >= 5):\n",
    "#         columns_to_drop.append(column)\n",
    "# for i in range(len(dff)):\n",
    "#     dff[i] = dff[i].drop(columns=columns_to_drop)\n",
    "#再对齐一下列，不知道为什么concat后还是没对齐：\n",
    "for i in range(len(dff)):\n",
    "    dffval[i] = dffval[i][dffval[i].columns.intersection(dfmval.columns)]\n",
    "\n",
    "dfftest=[]\n",
    "for df in dfs:  #删除测试集时间段内未上市的\n",
    "    df_filled = df[(df.index < 20210601) & (df.index>=20210101)]\n",
    "    df_filled=df_filled.dropna(axis=1,how='all')\n",
    "    df_filled = df_filled.fillna(method='bfill')\n",
    "    dfftest.append(df_filled)\n",
    "for i in range(len(dff)):\n",
    "    dfftest[i] = dfftest[i][dfftest[i].columns.intersection(dfmm.columns)]\n",
    "dfmmtest=pd.concat(dfftest,axis=0,join='inner')\n",
    "for i in range(len(dfftest)):\n",
    "    dfftest[i] = dfftest[i][dfftest[i].columns.intersection(dfmmtest.columns)]\n",
    "\n",
    "#训练集与验证集,测试集股票对齐\n",
    "dfmmm=pd.concat([dfftest[0],dff[0],dffval[0]],axis=0,join='inner')\n",
    "for i in range(len(dff)):\n",
    "    dff[i] = dff[i][dff[i].columns.intersection(dfmmm.columns)]\n",
    "for i in range(len(dff)):\n",
    "    dfftest[i] = dfftest[i][dfftest[i].columns.intersection(dfmmm.columns)]\n",
    "for i in range(len(dff)):\n",
    "    dffval[i] = dffval[i][dffval[i].columns.intersection(dfmmm.columns)]\n",
    "\n",
    "#装入tensor\n",
    "time_length =2431\n",
    "stock_code_length = 3726\n",
    "T=60\n",
    "# 创建一个空的三维数组，用于存放合并后的数据\n",
    "X_tensor = np.zeros((time_length, stock_code_length, 6))\n",
    "\n",
    "# 合并6个DataFrame的数据\n",
    "for i, df in enumerate(dff):\n",
    "    # 将DataFrame的值复制到对应的tensor切片中\n",
    "    X_tensor[:, :, i] = df.values\n",
    "X_tensor=torch.Tensor(X_tensor)\n",
    "total_samples = 2372  # 指定总样本数量\n",
    "time_steps_per_sample = 60  # 指定每个样本的时间步数\n",
    "num_stocks = 3726  # 股票数量\n",
    "num_features = 6  # 特征数量\n",
    "\n",
    "# 初始化新的X_train\n",
    "X_train = torch.zeros((total_samples, time_steps_per_sample, num_stocks, num_features))\n",
    "\n",
    "# 将数据拆分成样本\n",
    "\n",
    "for i in range(total_samples):\n",
    "    end_idx = i + time_steps_per_sample\n",
    "    X_train[i] = X_tensor[i:end_idx]\n",
    "\n",
    "time_length =243\n",
    "stock_code_length = 3726\n",
    "T=60\n",
    "# 创建一个空的三维数组，用于存放合并后的数据\n",
    "X_tensorval = np.zeros((time_length, stock_code_length, 6))\n",
    "# 合并6个DataFrame的数据\n",
    "for i, df in enumerate(dffval):\n",
    "    # 将DataFrame的值复制到对应的tensor切片中\n",
    "    X_tensorval[:, :, i] = df.values\n",
    "X_tensorval=torch.Tensor(X_tensorval)\n",
    "total_samples = 184  # 指定总样本数量\n",
    "# 初始化新的X_train\n",
    "X_val = torch.zeros((total_samples, time_steps_per_sample, num_stocks, num_features))\n",
    "# 将数据拆分成样本\n",
    "for i in range(total_samples):\n",
    "    end_idx = i + time_steps_per_sample\n",
    "    X_val[i] = X_tensorval[i:end_idx]\n",
    "\n",
    "\n",
    "time_length =97\n",
    "stock_code_length = 3726\n",
    "T=60\n",
    "# 创建一个空的三维数组，用于存放合并后的数据\n",
    "X_tensortest = np.zeros((time_length, stock_code_length, 6))\n",
    "# 合并6个DataFrame的数据\n",
    "for i, df in enumerate(dfftest):\n",
    "    # 将DataFrame的值复制到对应的tensor切片中\n",
    "    X_tensortest[:, :, i] = df.values\n",
    "X_tensortest=torch.Tensor(X_tensortest)\n",
    "total_samples = 38  # 指定总样本数量\n",
    "# 初始化新的X_train\n",
    "X_test = torch.zeros((total_samples, time_steps_per_sample, num_stocks, num_features))\n",
    "# 将数据拆分成样本\n",
    "\n",
    "for i in range(total_samples):\n",
    "    end_idx = i + time_steps_per_sample\n",
    "    X_test[i] = X_tensortest[i:end_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "close=pd.read_csv('/data/disk3/DataBase_stocks/AllSample/adjclose.csv',index_col=0)\n",
    "close.drop([x for x in close.columns if x[-2:]=='BJ'],axis=1,inplace=True)\n",
    "ret=(close-close.shift(1))/close.shift(1)\n",
    "ret= ret.fillna(method='bfill')\n",
    "ret = ret.apply(normalize_row, axis=1) #标准化\n",
    "\n",
    "ret1=ret[(ret.index <=20200102) & (ret.index>=20100101)]\n",
    "ret1= ret1[ret1.columns.intersection(dff[0].columns)]\n",
    "ret1=ret1.iloc[T:,]\n",
    "array = ret1.to_numpy()  # 将DataFrame转换为NumPy数组\n",
    "Y_train = np.reshape(array, ret1.shape)  # 将数组reshape为张量\n",
    "Y_train=torch.Tensor(Y_train)\n",
    "\n",
    "ret2=ret[(ret.index <=20210104) & (ret.index>20200101)]\n",
    "ret2= ret2[ret2.columns.intersection(dff[0].columns)]\n",
    "ret2=ret2.iloc[T:,]\n",
    "array = ret2.to_numpy()  # 将DataFrame转换为NumPy数组\n",
    "Y_val = np.reshape(array, ret2.shape)  # 将数组reshape为张量\n",
    "Y_val=torch.Tensor(Y_val)\n",
    "\n",
    "ret3=ret[(ret.index < 20210602) & (ret.index>=20210101)]\n",
    "ret3= ret3[ret3.columns.intersection(dff[0].columns)]\n",
    "ret3=ret3.iloc[T:,]\n",
    "array = ret3.to_numpy()  # 将DataFrame转换为NumPy数组\n",
    "Y_test = np.reshape(array, ret3.shape)  # 将数组reshape为张量\n",
    "Y_test=torch.Tensor(Y_test)\n",
    "\n",
    "window_size = 10  # 定义滑动窗口的大小，即连续的天数\n",
    "\n",
    "# 生成新的训练集标签\n",
    "ret1=ret[(ret.index <=20200115) & (ret.index>=20100101)]\n",
    "ret1= ret1[ret1.columns.intersection(dff[0].columns)]\n",
    "ret1=ret1.iloc[T:,]\n",
    "Y_train_new = []\n",
    "for i in range(len(ret1) - window_size + 1):\n",
    "    window = ret1[i:i + window_size]\n",
    "    Y_train_new.append(window)\n",
    "Y_train_new = np.stack(Y_train_new)  # 转换为NumPy数组\n",
    "Y_train_new = torch.Tensor(Y_train_new)\n",
    "\n",
    "ret3=ret[(ret.index <= 20210615) & (ret.index>=20210101)]\n",
    "ret3= ret3[ret3.columns.intersection(dff[0].columns)]\n",
    "ret3=ret3.iloc[T:,]\n",
    "Y_test_new = []\n",
    "for i in range(len(ret3) - window_size + 1):\n",
    "    window = ret3[i:i + window_size]\n",
    "    Y_test_new.append(window)\n",
    "Y_test_new = np.stack(Y_test_new)  # 转换为NumPy数组\n",
    "Y_test_new = torch.Tensor(Y_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 60, 3726, 6])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多因子预测\n",
    "class MyModelG(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,num_layers,dropout, num_factors, num_stocks):\n",
    "        super(MyModelG, self).__init__()\n",
    "        self.num_factors = num_factors\n",
    "        self.num_stocks = num_stocks\n",
    "        \n",
    "        # 定义LSTM层\n",
    "        self.GRU = nn.GRU(input_size, hidden_size, num_layers=num_layers,dropout=dropout,batch_first=True)\n",
    "        \n",
    "        # 定义全连接层\n",
    "        self.fc = nn.Linear(hidden_size, num_factors)  #h\n",
    "        \n",
    "        # 定义批标准化层\n",
    "        self.bn = nn.BatchNorm1d(num_factors)  #z\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, T, _, num_features = x.size()\n",
    "        \n",
    "        # 将num_stocks维度移到batch_size之后\n",
    "        x = x.view(-1, T, num_features)\n",
    "        \n",
    "        # GRU\n",
    "        lstm_out, _ = self.GRU(x)\n",
    "        \n",
    "        # 最后一个时间步的输出\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # 全连接层\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        \n",
    "        # 批标准化层\n",
    "        factor_output = self.bn(fc_out)\n",
    "        \n",
    "        # 将结果恢复成(batch_size, num_stocks, num_factors)形状\n",
    "        factor_output = factor_output.view(batch_size, self.num_stocks, self.num_factors)\n",
    "        \n",
    "        # 计算c，这里直接求因子平均\n",
    "        c = factor_output.mean(dim=2)\n",
    "        \n",
    "        return factor_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多因子预测\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,num_layers,dropout, num_factors, num_stocks):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.num_factors = num_factors\n",
    "        self.num_stocks = num_stocks\n",
    "        \n",
    "        # 定义LSTM层\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers=2,batch_first=True)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,dropout=dropout,batch_first=True)\n",
    "        \n",
    "        # 定义全连接层\n",
    "        self.fc = nn.Linear(hidden_size, num_factors)  #h\n",
    "        \n",
    "        # 定义批标准化层\n",
    "        self.bn = nn.BatchNorm1d(num_factors)  #z\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, T, _, num_features = x.size()\n",
    "        \n",
    "        # 将num_stocks维度移到batch_size之后\n",
    "        x = x.view(-1, T, num_features)\n",
    "        \n",
    "        # LSTM层\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # 最后一个时间步的输出\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # 全连接层\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        \n",
    "        # 批标准化层\n",
    "        factor_output = self.bn(fc_out)\n",
    "        \n",
    "        # 将结果恢复成(batch_size, num_stocks, num_factors)形状\n",
    "        factor_output = factor_output.view(batch_size, self.num_stocks, self.num_factors)\n",
    "        \n",
    "        # 计算c，这里直接求因子平均\n",
    "        c = factor_output.mean(dim=2)\n",
    "        \n",
    "        return factor_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测未来10日\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,num_layers,dropout, num_factors, num_stocks,preday):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.num_factors = num_factors\n",
    "        self.num_stocks = num_stocks\n",
    "        self.preday=preday\n",
    "        \n",
    "        # 定义LSTM层\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,dropout=dropout,batch_first=True)\n",
    "        \n",
    "        # 定义全连接层\n",
    "        self.fc = nn.Linear(hidden_size, num_factors)  #h\n",
    "        \n",
    "        # 定义批标准化层\n",
    "        self.bn = nn.BatchNorm1d(num_factors)  #z\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, T, _, num_features = x.size()\n",
    "        # 将num_stocks维度移到batch_size之后\n",
    "        x = x.view(-1, T, num_features)\n",
    "        # LSTM层\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # 最后10个时间步的输出\n",
    "        lstm_out = lstm_out[:, -self.preday:, :]\n",
    "        # 全连接层\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        # 批标准化层\n",
    "        fc_out=fc_out.view(batch_size*self.num_stocks*self.preday,self.num_factors)\n",
    "        factor_output = self.bn(fc_out)\n",
    "        # 将结果恢复成(batch_size, num_stocks, num_factors)形状\n",
    "        factor_output = factor_output.view(batch_size, self.num_stocks,self.preday, self.num_factors)\n",
    "    \n",
    "        c = factor_output.mean(dim=3)\n",
    "\n",
    "        return factor_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11999999999999995\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "stocks = 5\n",
    "\n",
    "y = torch.rand(batch_size, stocks)\n",
    "c = torch.rand(batch_size, stocks)\n",
    "\n",
    "# 将 Torch 张量转换为 NumPy 数组，以便使用 SciPy 函数\n",
    "y_numpy = y.numpy()\n",
    "c_numpy = c.numpy()\n",
    "\n",
    "# 初始化一个存储所有样本的相关系数的列表\n",
    "all_corr = []\n",
    "\n",
    "# 计算每个样本的斯皮尔曼相关系数，并将结果添加到列表中\n",
    "for i in range(batch_size):\n",
    "    spearman_corr, _ = spearmanr(y_numpy[i], c_numpy[i])\n",
    "    all_corr.append(spearman_corr)\n",
    "# 计算每个批次的平均斯皮尔曼相关系数\n",
    "batch_average_corr = np.mean(all_corr)\n",
    "\n",
    "print(batch_average_corr)\n",
    "# 假设你有一个形状为 (batch_size, stocks, 64) 的张量 tensor\n",
    "x=torch.rand(4,400,64)\n",
    "# 初始化一个空的DataFrame\n",
    "df_list = []\n",
    "# 按照 a 遍历Tensor并将每个二维数组转换为DataFrame的一行\n",
    "for i in range(x.shape[0]):\n",
    "    # 获取当前二维数组\n",
    "    current_array = x[i].numpy()\n",
    "    \n",
    "    # 将二维数组转换为DataFrame的一行，并添加到列表中\n",
    "    current_df = pd.DataFrame(current_array)\n",
    "    df_list.append(current_df)\n",
    "\n",
    "# 使用 concat 函数将所有DataFrame连接成一个\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "cor=df.corr()\n",
    "cor=cor*cor\n",
    "cor=cor.replace(1,0)\n",
    "penalty=cor.values.sum()/2\n",
    "\n",
    "print(penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_new(y, factor_output):\n",
    "    c = factor_output.mean(dim=2)  #因子等权求和\n",
    "    corr=[]\n",
    "    for i in range(c.shape[0]): #batch内循环\n",
    "        ct=c[i].T #stocks*preday\n",
    "        cin=[]\n",
    "        for j in range(c.shape[2]):\n",
    "            mean_c = torch.mean(ct[j])\n",
    "            mean_y = torch.mean(y[i][j])\n",
    "            \n",
    "            # Calculate the numerator and denominators for Pearson correlation\n",
    "            numerator = torch.sum((ct[j] - mean_c) * (y[i][j] - mean_y))\n",
    "            denominator_c = torch.sqrt(torch.sum((ct[j] - mean_c)**2))\n",
    "            denominator_y = torch.sqrt(torch.sum((y[i][j] - mean_y)**2))\n",
    "            \n",
    "            # Calculate the Pearson correlation coefficient\n",
    "            pearson_corr = numerator / (denominator_c * denominator_y)\n",
    "            cin.append(pearson_corr)\n",
    "        corr.append(torch.stack(cin).mean())\n",
    "    corr=torch.stack(corr).mean()\n",
    "    print(corr)\n",
    "    pen=[]\n",
    "    for i in range(factor_output.shape[0]): #batch内循环\n",
    "        penalty=[]\n",
    "        tt= factor_output[i].transpose(0, 1)\n",
    "        for j in range(tt.shape[0]): #天数内循环\n",
    "            correlation_matrix = torch.corrcoef(tt[j].T)\n",
    "            # 将对角线元素设置为零，即将每列与自身的相关系数剔除\n",
    "            for i in range(len(correlation_matrix)):\n",
    "                correlation_matrix[i, i] = 0\n",
    "            # 计算相关系数的平方和\n",
    "            zsum = torch.sum(correlation_matrix**2)\n",
    "            penalty.append(zsum)\n",
    "        pen.append(torch.stack(penalty).mean())\n",
    "    pen=torch.stack(pen).mean()\n",
    "    print(pen)\n",
    "    loss=-corr+0.1*pen\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据准备：假设有num_stocks只股票，每只股票有num_features个特征\n",
    "num_stocks = 3726\n",
    "num_features = 6\n",
    "T = 60  #时间步\n",
    "num_epochs = 30\n",
    "batch_size = 4\n",
    "\n",
    "# 创建数据，X_train的形状应为(训练集总长, T, num_stocks, num_features)，y_train的形状应为(训练集总长, num_stocks)\n",
    "# 模型构建\n",
    "\n",
    "# 创建模型实例\n",
    "input_size = num_features\n",
    "hidden_size = 64\n",
    "num_layers=1\n",
    "dropout=0.1\n",
    "num_factors = 60  #60个等权输出-天数？\n",
    "preday=10\n",
    "model = MyModel(input_size, hidden_size, num_layers,dropout,num_factors, num_stocks,preday)\n",
    "best_spearman_corr=-1\n",
    "# model = DataParallel(model)\n",
    "model.to(device)  # 将模型移动到GPU\n",
    "\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 定义学习率调度器，以在第10轮后降低学习率\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=1)\n",
    "count=0\n",
    "# 训练模型\n",
    "loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    torch.cuda.empty_cache() \n",
    "    loss_epoch = []\n",
    "    for i in range(0, X_train.size(0)-4, batch_size):\n",
    "        inputs = X_train[i:i+batch_size].to(device)\n",
    "        labels = Y_train_new[i:i+batch_size].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 计算损失函数\n",
    "        loss = custom_loss_new(labels, outputs)\n",
    "            \n",
    "        print(epoch, i, loss.item()/num_stocks)\n",
    "        loss_epoch.append(loss.item()/num_stocks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_history.append(np.mean(loss_epoch))\n",
    "    \n",
    "    # 调度学习率\n",
    "    scheduler.step()\n",
    "    # 打印当前学习率\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch + 1} - Learning Rate: {current_lr}\")\n",
    "\n",
    "    #若loss比前一轮大则结束\n",
    "    if epoch>=3 :\n",
    "        if (loss.item()/num_stocks)>last:\n",
    "            count+=1\n",
    "            if count>=3:\n",
    "                print('不收敛')\n",
    "                break\n",
    "    last=loss.item()/num_stocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4088, 500])\n",
      "torch.Size([4, 4088])\n",
      "tensor(104.9805, grad_fn=<AddBackward0>)\n",
      "tensor(104.9805, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mo=MyModelZJ(input_size, num_factors, num_stocks)\n",
    "y=mo(X[:4])\n",
    "print(y.shape)\n",
    "label=Y[:4]\n",
    "print(label.shape)\n",
    "print(custom_loss(label,y,1))\n",
    "torch.any(torch.isnan(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y, factor_output,k):  #惩罚项系数k\n",
    "    c = factor_output.mean(dim=2)\n",
    "    corr=[]\n",
    "    for i in range(c.shape[0]): #batch内循环\n",
    "        # # 计算秩次差值\n",
    "        # rank_X = torch.argsort(c[i].reshape(-1))\n",
    "        # rank_Y = torch.argsort(y[i].reshape(-1))\n",
    "        # differences = rank_X - rank_Y\n",
    "        # # 计算斯皮尔曼秩相关系数\n",
    "        # n = len(c[i])\n",
    "        # spearman_corr = 1 - (6 * torch.sum(differences**2)) / (n * (n**2 - 1))\n",
    "        # corr.append(spearman_corr)\n",
    "        mean_c = torch.mean(c[i])\n",
    "        mean_y = torch.mean(y[i])\n",
    "        \n",
    "        # Calculate the numerator and denominators for Pearson correlation\n",
    "        numerator = torch.sum((c[i] - mean_c) * (y[i] - mean_y))\n",
    "        denominator_c = torch.sqrt(torch.sum((c[i] - mean_c)**2))\n",
    "        denominator_y = torch.sqrt(torch.sum((y[i] - mean_y)**2))\n",
    "        # Calculate the Pearson correlation coefficient\n",
    "        pearson_corr = numerator / (denominator_c * denominator_y)\n",
    "        corr.append(pearson_corr)\n",
    "    corr=torch.stack(corr).mean()\n",
    "    penalty=[]\n",
    "    for i in range(factor_output.shape[0]): #batch内循环\n",
    "        correlation_matrix = torch.corrcoef(factor_output[i].T)\n",
    "        # 将对角线元素设置为零，即将每列与自身的相关系数剔除\n",
    "        for i in range(factor_output.shape[2]):\n",
    "            correlation_matrix[i, i] = 0\n",
    "        # 计算相关系数矩阵的L2范数\n",
    "        zsum = torch.sum(correlation_matrix**2)\n",
    "        penalty.append(zsum)\n",
    "    penalty=torch.stack(penalty).mean().sqrt()\n",
    "    loss=-corr+k*penalty\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4088, 500])\n",
      "torch.Size([4, 4088])\n",
      "tensor(1.0806, grad_fn=<AddBackward0>)\n",
      "tensor(1.0806, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = MyModelZJ(4321,500,4088)\n",
    "x=torch.rand(4,4088,4321)\n",
    "y=model(x)\n",
    "label=torch.rand(4,4088)\n",
    "print(y.shape)\n",
    "c = y.mean(dim=2)\n",
    "print(c.shape)\n",
    "print(custom_loss(label, y,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 400, 1])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([400, 60])\n",
      "torch.Size([60, 60])\n"
     ]
    }
   ],
   "source": [
    "y=torch.rand(4,400,1)\n",
    "factor_output=torch.rand(4,400,60)\n",
    "c = factor_output.mean(dim=2)\n",
    "print(y.shape)\n",
    "# print(factor_output.shape)\n",
    "corr=[]\n",
    "for i in range(c.shape[0]): #batch内循环\n",
    "    print(c[i].shape)\n",
    "    mean_c = torch.mean(c[i])\n",
    "    mean_y = torch.mean(y[i])\n",
    "    \n",
    "    # Calculate the numerator and denominators for Pearson correlation\n",
    "    numerator = torch.sum((c[i] - mean_c) * (y[i] - mean_y))\n",
    "    denominator_c = torch.sqrt(torch.sum((c[i] - mean_c)**2))\n",
    "    denominator_y = torch.sqrt(torch.sum((y[i] - mean_y)**2))\n",
    "    \n",
    "    # Calculate the Pearson correlation coefficient\n",
    "    pearson_corr = numerator / (denominator_c * denominator_y)\n",
    "    corr.append(pearson_corr)\n",
    "corr=torch.stack(corr).mean()\n",
    "\n",
    "factor_output=torch.rand(4,400,60)\n",
    "penalty=[]\n",
    "for i in range(factor_output.shape[0]): #batch内循环\n",
    "    print(factor_output[i].shape)\n",
    "    correlation_matrix = torch.corrcoef(factor_output[i].T)\n",
    "    print(correlation_matrix.shape)\n",
    "    # 将对角线元素设置为零，即将每列与自身的相关系数剔除\n",
    "    for i in range(factor_output.shape[2]):\n",
    "        correlation_matrix[i, i] = 0\n",
    "    # 计算相关系数的平方和\n",
    "    zsum = torch.sum(correlation_matrix**2)\n",
    "    penalty.append(zsum)\n",
    "    break\n",
    "penalty=torch.stack(penalty).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # 释放显存\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss=0.007590079489001037- Learning Rate: 0.01\n",
      "Epoch 2 loss=0.005312651772943884- Learning Rate: 0.01\n",
      "Epoch 3 loss=0.004362682903245834- Learning Rate: 0.01\n",
      "Epoch 4 loss=0.0037513339900534996- Learning Rate: 0.01\n",
      "Epoch 5 loss=0.0032049332320314306- Learning Rate: 0.01\n",
      "Epoch 6 loss=0.0027842954180245295- Learning Rate: 0.01\n",
      "Epoch 7 loss=0.0024472525234350982- Learning Rate: 0.01\n",
      "Epoch 8 loss=0.002153436140693719- Learning Rate: 0.01\n",
      "Epoch 9 loss=0.0018750470574717595- Learning Rate: 0.01\n",
      "Epoch 10 loss=0.0016326764584313428- Learning Rate: 0.01\n",
      "Epoch 11 loss=0.0014332966613728058- Learning Rate: 0.01\n",
      "Epoch 12 loss=0.0012682067334431623- Learning Rate: 0.01\n",
      "Epoch 13 loss=0.0011298045746060295- Learning Rate: 0.01\n",
      "Epoch 14 loss=0.001014724401682194- Learning Rate: 0.01\n",
      "Epoch 15 loss=0.0009193494608153931- Learning Rate: 0.01\n",
      "Epoch 16 loss=0.0008382616820193342- Learning Rate: 0.01\n",
      "Epoch 17 loss=0.0007676017831320035- Learning Rate: 0.01\n",
      "Epoch 18 loss=0.0007062389037224957- Learning Rate: 0.01\n",
      "Epoch 19 loss=0.0006542102011112212- Learning Rate: 0.01\n",
      "Epoch 20 loss=0.0006135320088012656- Learning Rate: 0.01\n",
      "Epoch 21 loss=0.0006041906571538586- Learning Rate: 0.01\n",
      "Epoch 22 loss=0.0006009602603717429- Learning Rate: 0.01\n",
      "Epoch 23 loss=0.0005713863706142909- Learning Rate: 0.01\n",
      "Epoch 24 loss=0.0005307529900068303- Learning Rate: 0.01\n",
      "Epoch 25 loss=0.0004942473781500258- Learning Rate: 0.01\n",
      "Epoch 26 loss=0.0004644453415380247- Learning Rate: 0.01\n",
      "Epoch 27 loss=0.0004422052376009531- Learning Rate: 0.01\n",
      "Epoch 28 loss=0.00042331450320917983- Learning Rate: 0.01\n",
      "Epoch 29 loss=0.00040569104988125523- Learning Rate: 0.01\n",
      "Epoch 30 loss=0.0003890352006527775- Learning Rate: 0.01\n",
      "Epoch 31 loss=0.000373185459855692- Learning Rate: 0.01\n",
      "Epoch 32 loss=0.0003580645572416003- Learning Rate: 0.01\n",
      "Epoch 33 loss=0.00034362863304563697- Learning Rate: 0.01\n",
      "Epoch 34 loss=0.0003298578877427262- Learning Rate: 0.01\n",
      "Epoch 35 loss=0.00031674416279321027- Learning Rate: 0.01\n",
      "Epoch 36 loss=0.00030430202498128034- Learning Rate: 0.01\n",
      "Epoch 37 loss=0.00029252963393738694- Learning Rate: 0.01\n",
      "Epoch 38 loss=0.00028143550137069644- Learning Rate: 0.01\n",
      "Epoch 39 loss=0.0002709848922500146- Learning Rate: 0.01\n",
      "Epoch 40 loss=0.0002611906052439284- Learning Rate: 0.01\n",
      "Epoch 41 loss=0.0002520391204404323- Learning Rate: 0.01\n",
      "Epoch 42 loss=0.00024356353889472802- Learning Rate: 0.01\n",
      "Epoch 43 loss=0.00023584578718457907- Learning Rate: 0.01\n",
      "Epoch 44 loss=0.0002286661038012006- Learning Rate: 0.01\n",
      "Epoch 45 loss=0.00022222869427055347- Learning Rate: 0.01\n",
      "Epoch 46 loss=0.00021649508838887888- Learning Rate: 0.01\n",
      "Epoch 47 loss=0.00021243738198855775- Learning Rate: 0.01\n",
      "Epoch 48 loss=0.00022283171727414182- Learning Rate: 0.01\n",
      "Epoch 49 loss=0.00026776589798289653- Learning Rate: 0.01\n",
      "Epoch 50 loss=0.00029225034457389413- Learning Rate: 0.01\n",
      "Epoch 51 loss=0.00028701572379072636- Learning Rate: 0.01\n",
      "Epoch 52 loss=0.00026422825150449364- Learning Rate: 0.01\n",
      "Epoch 53 loss=0.000239079405909441- Learning Rate: 0.01\n",
      "Epoch 54 loss=0.0002335548984094609- Learning Rate: 0.01\n",
      "Epoch 55 loss=0.0002368391460894398- Learning Rate: 0.01\n",
      "Epoch 56 loss=0.0002408685325072833- Learning Rate: 0.01\n",
      "Epoch 57 loss=0.0002375049276387181- Learning Rate: 0.01\n",
      "Epoch 58 loss=0.00022926916795389682- Learning Rate: 0.01\n",
      "Epoch 59 loss=0.00022231907520793941- Learning Rate: 0.01\n",
      "Epoch 60 loss=0.00021384132733990975- Learning Rate: 0.01\n",
      "Epoch 61 loss=0.00020677265355731645- Learning Rate: 0.01\n",
      "Epoch 62 loss=0.0001987120481282893- Learning Rate: 0.01\n",
      "Epoch 63 loss=0.0001916521838188793- Learning Rate: 0.01\n",
      "Epoch 64 loss=0.00018776647518136807- Learning Rate: 0.01\n",
      "Epoch 65 loss=0.00018529033927871857- Learning Rate: 0.01\n",
      "Epoch 66 loss=0.00018326830808223138- Learning Rate: 0.01\n",
      "Epoch 67 loss=0.00018150385669273613- Learning Rate: 0.01\n",
      "Epoch 68 loss=0.00017996265703399536- Learning Rate: 0.01\n",
      "Epoch 69 loss=0.00017861873267862845- Learning Rate: 0.01\n",
      "Epoch 70 loss=0.00017743124815919496- Learning Rate: 0.01\n",
      "Epoch 71 loss=0.00017633731467849825- Learning Rate: 0.01\n",
      "Epoch 72 loss=0.0001753055020943235- Learning Rate: 0.01\n",
      "Epoch 73 loss=0.0001743242808802539- Learning Rate: 0.01\n",
      "Epoch 74 loss=0.00017338685560133125- Learning Rate: 0.01\n",
      "Epoch 75 loss=0.00017248884403957027- Learning Rate: 0.01\n",
      "Epoch 76 loss=0.00017162754590626931- Learning Rate: 0.01\n",
      "Epoch 77 loss=0.00017079987923045034- Learning Rate: 0.01\n",
      "Epoch 78 loss=0.000170003434035228- Learning Rate: 0.01\n",
      "Epoch 79 loss=0.00016923584991705162- Learning Rate: 0.01\n",
      "Epoch 80 loss=0.00016849380870258377- Learning Rate: 0.01\n",
      "Epoch 81 loss=0.00016777406998058072- Learning Rate: 0.01\n",
      "Epoch 82 loss=0.00016707517085165586- Learning Rate: 0.01\n",
      "Epoch 83 loss=0.00016639433358902255- Learning Rate: 0.01\n",
      "Epoch 84 loss=0.00016573146617420345- Learning Rate: 0.01\n",
      "Epoch 85 loss=0.0001650859824754176- Learning Rate: 0.01\n",
      "Epoch 86 loss=0.00016445633632303659- Learning Rate: 0.01\n",
      "Epoch 87 loss=0.00016384281381676315- Learning Rate: 0.01\n",
      "Epoch 88 loss=0.00016324220143807766- Learning Rate: 0.01\n",
      "Epoch 89 loss=0.00016265099017250456- Learning Rate: 0.01\n",
      "Epoch 90 loss=0.00016206850219379433- Learning Rate: 0.01\n",
      "Epoch 91 loss=0.000161495806082717- Learning Rate: 0.01\n",
      "Epoch 92 loss=0.00016093437802301697- Learning Rate: 0.01\n",
      "Epoch 93 loss=0.00016038654083323287- Learning Rate: 0.01\n",
      "Epoch 94 loss=0.00015985123079272553- Learning Rate: 0.01\n",
      "Epoch 95 loss=0.0001593274901317083- Learning Rate: 0.01\n",
      "Epoch 96 loss=0.0001588144968400496- Learning Rate: 0.01\n",
      "Epoch 97 loss=0.0001583130298347208- Learning Rate: 0.01\n",
      "Epoch 98 loss=0.00015782085475156453- Learning Rate: 0.01\n",
      "Epoch 99 loss=0.00015734008542349297- Learning Rate: 0.01\n",
      "Epoch 100 loss=0.00015686602372401535- Learning Rate: 0.01\n",
      "Epoch 101 loss=0.00015640604473969602- Learning Rate: 0.01\n",
      "Epoch 102 loss=0.0001559503335982827- Learning Rate: 0.01\n",
      "Epoch 103 loss=0.00015552285204091317- Learning Rate: 0.01\n",
      "Epoch 104 loss=0.00015510628471739475- Learning Rate: 0.01\n",
      "Epoch 105 loss=0.0001547603461114602- Learning Rate: 0.01\n",
      "Epoch 106 loss=0.00015438561576634654- Learning Rate: 0.01\n",
      "Epoch 107 loss=0.00015406228519310302- Learning Rate: 0.01\n",
      "Epoch 108 loss=0.0001536369783822856- Learning Rate: 0.01\n",
      "Epoch 109 loss=0.00015331937563525618- Learning Rate: 0.01\n",
      "Epoch 110 loss=0.00015287498114652855- Learning Rate: 0.01\n",
      "Epoch 111 loss=0.0001525486716370812- Learning Rate: 0.01\n",
      "Epoch 112 loss=0.00015210774209764268- Learning Rate: 0.01\n",
      "Epoch 113 loss=0.00015180770969567132- Learning Rate: 0.01\n",
      "Epoch 114 loss=0.00015141608423967933- Learning Rate: 0.01\n",
      "Epoch 115 loss=0.00015126644210676496- Learning Rate: 0.01\n",
      "Epoch 116 loss=0.0001516291238634865- Learning Rate: 0.01\n",
      "Epoch 117 loss=0.00019123264209527926- Learning Rate: 0.01\n",
      "不收敛\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train=X[:184]\n",
    "Y_train=Y[:184]\n",
    "# 数据准备：假设有num_stocks只股票，每只股票有num_features个特征\n",
    "# num_stocks = 4470\n",
    "# num_features = 4320\n",
    "num_epochs = 120\n",
    "batch_size = 4\n",
    "k=0.1 #惩罚项系数\n",
    "# 创建数据，X_train的形状应为(训练集总长, T, num_stocks, num_features)，y_train的形状应为(训练集总长, num_stocks)\n",
    "# 模型构建\n",
    "\n",
    "# 创建模型实例\n",
    "input_size = num_features\n",
    "num_factors = 400  #60个等权输出-天数？\n",
    "\n",
    "model = MyModelZJ(input_size, num_factors, num_stocks)\n",
    "model.to(device)  # 将模型移动到GPU\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 定义学习率调度器，以在第10轮后降低学习率\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=40, gamma=1)\n",
    "\n",
    "# 训练模型\n",
    "loss_history = []\n",
    "count=0\n",
    "for epoch in range(num_epochs):\n",
    "    torch.cuda.empty_cache() \n",
    "    loss_epoch = []\n",
    "    for i in range(0, X_train.size(0)-batch_size, batch_size):\n",
    "        inputs = X_train[i:i+batch_size].to(device)\n",
    "        labels = Y_train[i:i+batch_size].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失函数\n",
    "        loss = custom_loss(labels, outputs,k)\n",
    "        # print(loss)\n",
    "        # print(epoch, i, loss.item()/num_stocks)\n",
    "        loss_epoch.append(loss.item()/num_stocks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_history.append(np.mean(loss_epoch))\n",
    "    \n",
    "    # 调度学习率\n",
    "    scheduler.step()\n",
    "    # 打印当前学习率\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch + 1} loss={np.mean(loss_epoch)}- Learning Rate: {current_lr}\")\n",
    "\n",
    "        #若loss比前一轮大则结束\n",
    "    if epoch>=3 :\n",
    "        if (loss.item()/num_stocks)>last:\n",
    "            count+=1\n",
    "            if count>=3:\n",
    "                print('不收敛')\n",
    "                break\n",
    "    last=loss.item()/num_stocks\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据准备：假设有num_stocks只股票，每只股票有num_features个特征\n",
    "num_stocks = 3726\n",
    "num_features = 6\n",
    "T = 60  #时间步\n",
    "num_epochs = 30\n",
    "batch_size = 4\n",
    "\n",
    "# 创建数据，X_train的形状应为(训练集总长, T, num_stocks, num_features)，y_train的形状应为(训练集总长, num_stocks)\n",
    "# 模型构建\n",
    "\n",
    "# 创建模型实例\n",
    "input_size = num_features\n",
    "hidden_size = 64\n",
    "num_layers=3\n",
    "dropout=0.1\n",
    "num_factors = 60  #60个等权输出-天数？\n",
    "\n",
    "model = MyModelG(input_size, hidden_size, num_layers,dropout,num_factors, num_stocks)\n",
    "best_spearman_corr=-1\n",
    "model.to(device)  # 将模型移动到GPU\n",
    "\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 定义学习率调度器，以在第10轮后降低学习率\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=1)\n",
    "count=0\n",
    "# 训练模型\n",
    "loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    torch.cuda.empty_cache() \n",
    "    loss_epoch = []\n",
    "    for i in range(0, X_train.size(0)-4, batch_size):\n",
    "        inputs = X_train[i:i+batch_size].to(device)\n",
    "        labels = Y_train[i:i+batch_size].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 计算损失函数\n",
    "        loss = custom_loss(labels, outputs)\n",
    "            \n",
    "        print(epoch, i, loss.item()/num_stocks)\n",
    "        loss_epoch.append(loss.item()/num_stocks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_history.append(np.mean(loss_epoch))\n",
    "    \n",
    "    # 调度学习率\n",
    "    scheduler.step()\n",
    "    # 打印当前学习率\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch + 1} - Learning Rate: {current_lr}\")\n",
    "\n",
    "    #若loss比前一轮大则结束\n",
    "    if epoch>=3 :\n",
    "        if (loss.item()/num_stocks)>last:\n",
    "            count+=1\n",
    "            if count>=3:\n",
    "                print('不收敛')\n",
    "                break\n",
    "    last=loss.item()/num_stocks\n",
    "    # 在验证集上计算斯皮尔曼相关系数\n",
    "#     with torch.no_grad():\n",
    "#         torch.cuda.empty_cache() \n",
    "#         c = []\n",
    "#         for i in range(0, X_val.size(0)-4, batch_size):\n",
    "#             input_sample = X_val[i:i+batch_size].to(device)\n",
    "#             label_sample = Y_val[i:i+batch_size].to(device)\n",
    "#             # input_sample = X_val  # 获取单个样本并添加批次维度\n",
    "#             # label_sample = Y_val\n",
    "#             # 使用模型进行推理并获取单个样本的预测输出（c值）\n",
    "#             factors = model(input_sample)\n",
    "#             c_value=factors.mean(dim=2)\n",
    "#             print(c_value.shape)\n",
    "#             corr2=[]\n",
    "#             for i in range(c_value.shape[0]): \n",
    "#                 rank_X = torch.argsort(c_value[i].reshape(-1))\n",
    "#                 rank_Y = torch.argsort(label_sample[i].reshape(-1))\n",
    "#                 differences = rank_X - rank_Y\n",
    "#                 # 计算斯皮尔曼秩相关系数\n",
    "#                 n = len(c_value[i])\n",
    "#                 spearman_corr = 1 - (6 * torch.sum(differences**2)) / (n * (n**2 - 1))\n",
    "#                 print(spearman_corr)\n",
    "#                 corr2.append(spearman_corr)\n",
    "#             # 更新最佳斯皮尔曼相关系数和模型权重\n",
    "#             corr2=sum(corr2)/len(corr2)\n",
    "#             c.append(corr2)\n",
    "#         c=sum(c)/len(c)\n",
    "#         if c > best_spearman_corr:\n",
    "#             best_spearman_corr = c\n",
    "#             best_model_weights = model.state_dict()\n",
    "#         print(best_spearman_corr)\n",
    "#     # 如果斯皮尔曼相关系数达到了某个停止条件，可以提前停止训练\n",
    "#     if best_spearman_corr >= 0.2:\n",
    "#         print(f\"Training stopped early at Epoch {epoch + 1} because of achieving the desired Spearman correlation.\")\n",
    "#         break\n",
    "\n",
    "# # 使用最佳模型权重\n",
    "# model.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    epochs = range(1, len(loss_history) + 1)\n",
    "    plt.plot(epochs, loss_history, marker='o', linestyle='-')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(loss_history)\n",
    "def plot_loss(loss_history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    epochs = range(3, len(loss_history) + 1)  # 从第二轮开始绘制，所以范围从2开始\n",
    "    plt.plot(epochs, loss_history[2:], marker='o', linestyle='-')  # 从第二轮开始绘制，所以从索引1开始\n",
    "    plt.title('Loss Over Epochs (Starting from Epoch 2)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# 调用修改后的函数以绘制损失图\n",
    "plot_loss(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "torch.save(model.state_dict(), '/data/disk4/output_stocks/jmchen/factors/ML/zj_model/20_4_400_0.01s.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 4121)\n"
     ]
    }
   ],
   "source": [
    "dfftest=[]\n",
    "for df in dfs:  #删除测试集时间段内未上市的\n",
    "    df_filled = df[(df.index < 20211231) & (df.index>=20210101)]\n",
    "    df_filled=df_filled.dropna(axis=1,how='all')\n",
    "    df_filled = df_filled.fillna(method='bfill')\n",
    "    dfftest.append(df_filled)\n",
    "for i in range(len(dff)):\n",
    "    dfftest[i] = dfftest[i][dfftest[i].columns.intersection(dfmm.columns)]\n",
    "print(dfftest[0].shape)\n",
    "time_length =242\n",
    "stock_code_length = 4121\n",
    "T=60\n",
    "# 创建一个空的三维数组，用于存放合并后的数据\n",
    "X_tensortest = np.zeros((time_length, stock_code_length, 6))\n",
    "\n",
    "# 合并6个DataFrame的数据\n",
    "for i, df in enumerate(dfftest):\n",
    "    # 将DataFrame的值复制到对应的tensor切片中\n",
    "    X_tensortest[:, :, i] = df.values\n",
    "X_tensortest=torch.Tensor(X_tensortest)\n",
    "total_samples = 183  # 指定总样本数量\n",
    "time_steps_per_sample = 60  # 指定每个样本的时间步数\n",
    "num_stocks = 4121  # 股票数量\n",
    "num_features = 6  # 特征数量\n",
    "\n",
    "# 初始化新的X_train\n",
    "X_test1 = torch.zeros((total_samples, time_steps_per_sample, num_stocks, num_features))\n",
    "\n",
    "# 将数据拆分成样本\n",
    "\n",
    "for i in range(total_samples):\n",
    "    end_idx = i + time_steps_per_sample\n",
    "    X_test1[i] = X_tensortest[i:end_idx]\n",
    "\n",
    "close=pd.read_csv('/data/disk3/DataBase_stocks/AllSample/adjclose.csv',index_col=0)\n",
    "close.drop([x for x in close.columns if x[-2:]=='BJ'],axis=1,inplace=True)\n",
    "ret=(close-close.shift(1))/close.shift(1)\n",
    "ret= ret.fillna(method='bfill')\n",
    "ret=ret[(ret.index <= 20211231) & (ret.index>=20210101)]\n",
    "ret= ret[ret.columns.intersection(dff[0].columns)]\n",
    "ret=ret.iloc[T:,]\n",
    "array = ret.to_numpy()  # 将DataFrame转换为NumPy数组\n",
    "Y_test1 = np.reshape(array, ret.shape)  # 将数组reshape为张量\n",
    "Y_test1=torch.Tensor(Y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 3726])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 4088])\n",
      "torch.Size([63, 4088])\n",
      "Pearson: 0.08805353831618078\n",
      "Spearman: 0.011032825\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test=X[180:]\n",
    "Y_test=Y[180:]\n",
    "model = MyModelZJ(input_size, num_factors, num_stocks)\n",
    "# 加载保存的模型参数\n",
    "model.load_state_dict(torch.load('/data/disk4/output_stocks/jmchen/factors/ML/zj_model/20_4_400_0.01s.pth'))\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_sample = X_test  # 获取单个样本并添加批次维度\n",
    "    label_sample = Y_test\n",
    "    # 使用模型进行推理并获取单个样本的预测输出（c值）\n",
    "    factors = model(input_sample)\n",
    "    c_value=factors.mean(dim=2)\n",
    "    print(c_value.shape)\n",
    "    print(label_sample.shape)\n",
    "    c1=[]\n",
    "    c2=[]\n",
    "    for i in range(c_value.shape[0]): \n",
    "        cv=c_value[i].T\n",
    "        corr1=[]\n",
    "        corr2=[]\n",
    "\n",
    "        # 创建遮罩以识别c_value和Y_test中的非NaN值\n",
    "        mask_c = ~torch.isnan(cv)\n",
    "        mask_y = ~torch.isnan(label_sample[i])\n",
    "        \n",
    "        # 组合这些遮罩以获取c_value和Y_test的共同遮罩\n",
    "        mask = mask_c & mask_y\n",
    "        \n",
    "        # 将遮罩应用于张量\n",
    "        c_value_filtered = cv[mask]\n",
    "        Y_test_filtered = label_sample[i][mask]\n",
    "\n",
    "    \n",
    "        # 计算每个输入的均值\n",
    "        mean_c = torch.mean(c_value_filtered)\n",
    "        mean_y = torch.mean(Y_test_filtered)\n",
    "        # print(mean_c,mean_y)\n",
    "        # 计算皮尔逊相关系数的分子和分母\n",
    "        numerator = torch.sum((c_value_filtered - mean_c) * (Y_test_filtered - mean_y))\n",
    "        # print(numerator)\n",
    "        denominator_c = torch.sqrt(torch.sum((c_value_filtered - mean_c)**2))\n",
    "        denominator_y = torch.sqrt(torch.sum((Y_test_filtered - mean_y)**2))\n",
    "\n",
    "        # 计算皮尔逊相关系数\n",
    "        pearson_corr = numerator / (denominator_c * denominator_y)\n",
    "        corr1.append(pearson_corr.item())\n",
    "        # 计算秩次差值\n",
    "        rank_X = torch.argsort(cv.reshape(-1))\n",
    "        rank_Y = torch.argsort(label_sample[i].reshape(-1))\n",
    "        differences = rank_X - rank_Y\n",
    "        # 计算斯皮尔曼秩相关系数\n",
    "        n = len(c_value[i])\n",
    "        spearman_corr = 1 - (6 * torch.sum(differences**2)) / (n * (n**2 - 1))\n",
    "        corr2.append(spearman_corr)\n",
    "\n",
    "        c1.append(np.mean(corr1))\n",
    "        c2.append(np.mean(corr2))\n",
    "    c1=np.mean(c1)\n",
    "    c2=np.mean(c2)\n",
    "    print('Pearson:',c1)\n",
    "    print('Spearman:',c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个与您训练模型相同的模型实例\n",
    "model = MyModel(input_size, hidden_size, num_layers,dropout,num_factors, num_stocks,preday)\n",
    "\n",
    "# 加载训练好的模型参数\n",
    "model.load_state_dict(torch.load('model_params.pth'))\n",
    "model.to(device)\n",
    "batch_size = 2  # 适当调整批次大小，以适应您的GPU内存\n",
    "num_samples = X_test.size(0)\n",
    "num_batches = (num_samples + batch_size - 1) // batch_size  # 向上取整\n",
    "\n",
    "# 初始化变量以跟踪正确的预测数\n",
    "correct_predictions = 0\n",
    "\n",
    "# 将模型设置为评估模式（不使用dropout等）\n",
    "model.eval()\n",
    "\n",
    "# 逐批次进行推断\n",
    "with torch.no_grad():  # 禁用梯度计算，因为在推断时不需要梯度\n",
    "    for batch_idx in range(num_batches):\n",
    "        # 获取当前批次的起始和结束索引\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, num_samples)\n",
    "\n",
    "        # 获取当前批次的输入数据和标签\n",
    "        batch_X = X_test[start_idx:end_idx]\n",
    "        batch_Y = Y_test[start_idx:end_idx]\n",
    "\n",
    "        # 将批次数据移动到GPU\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_Y = batch_Y.to(device)\n",
    "\n",
    "        # 进行模型推断\n",
    "        batch_outputs = model(batch_X)\n",
    "\n",
    "        # 计算当前批次的正确预测数\n",
    "        predicted_labels = torch.argmax(batch_outputs, dim=1)\n",
    "        correct_predictions += (predicted_labels == batch_Y).sum().item()\n",
    "\n",
    "# 计算整体准确率\n",
    "total_samples = num_samples\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据准备：假设有num_stocks只股票，每只股票有num_features个特征\n",
    "num_stocks = 4165\n",
    "num_features = 6\n",
    "T = 60  # 假设时间步为30\n",
    "num_epochs = 20\n",
    "batch_size = 2\n",
    "\n",
    "# 创建数据，X_train的形状应为(训练集总长, T, num_stocks, num_features)，y_train的形状应为(训练集总长, num_stocks)\n",
    "X_train\n",
    "Y_train \n",
    "\n",
    "# 模型构建\n",
    "\n",
    "# 创建模型实例\n",
    "input_size = num_features\n",
    "hidden_size = 64\n",
    "num_factors = 1  #60个等权输出？\n",
    "\n",
    "model = MyModel(input_size, hidden_size, num_factors, num_stocks)\n",
    "\n",
    "\n",
    "model = DataParallel(model)\n",
    "model.to(device)  # 将模型移动到GPU\n",
    "X_train = X_train.to(device)  # 将训练数据移动到GPU\n",
    "Y_train = Y_train.to(device)  # 将训练标签移动到GPU\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scaler = GradScaler()\n",
    "for epoch in range(num_epochs):\n",
    "    torch.cuda.empty_cache()  # 释放显存\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        inputs = X_train[i:i+batch_size]\n",
    "        labels = Y_train[i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 使用autocast开启混合精度计算\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = 0\n",
    "            for j in range(num_stocks):\n",
    "                mse_loss = F.mse_loss(outputs[:, j, :], labels[:, j], reduction='mean')\n",
    "                loss += mse_loss\n",
    "            \n",
    "            # 缩放损失值\n",
    "            loss = loss / num_stocks\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "        # 使用scaler.step来更新模型参数\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        print(epoch, i, loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>20211217_10_rank</th>\n",
       "      <th>20211217_12_rank</th>\n",
       "      <th>20211217_1_abs</th>\n",
       "      <th>20211217_1_abs_regwin=1_rank_abs</th>\n",
       "      <th>20211217_1_abs_regwin=20_rank_abs</th>\n",
       "      <th>20211217_1_abs_regwin=40_rank_abs</th>\n",
       "      <th>20211217_1_abs_regwin=5_rank_abs</th>\n",
       "      <th>20211217_1_ori_regwin=10_abs</th>\n",
       "      <th>...</th>\n",
       "      <th>market_leverage</th>\n",
       "      <th>natural_log_of_market_cap</th>\n",
       "      <th>non_linear_size</th>\n",
       "      <th>predicted_earnings_to_price_ratio</th>\n",
       "      <th>raw_beta</th>\n",
       "      <th>relative_strength</th>\n",
       "      <th>residual_volatility</th>\n",
       "      <th>sales_growth</th>\n",
       "      <th>share_turnover_monthly</th>\n",
       "      <th>short_term_predicted_earnings_growth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20200102</td>\n",
       "      <td>0.031403</td>\n",
       "      <td>1.644531</td>\n",
       "      <td>1.876953</td>\n",
       "      <td>1.681641</td>\n",
       "      <td>1.674805</td>\n",
       "      <td>1.673828</td>\n",
       "      <td>1.690430</td>\n",
       "      <td>1.885742</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.830078</td>\n",
       "      <td>-2.357422</td>\n",
       "      <td>1.259766</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>1.269531</td>\n",
       "      <td>-0.447510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.295898</td>\n",
       "      <td>-0.309814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20200102</td>\n",
       "      <td>-0.103943</td>\n",
       "      <td>1.631836</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>1.670898</td>\n",
       "      <td>1.126953</td>\n",
       "      <td>1.338867</td>\n",
       "      <td>1.666016</td>\n",
       "      <td>1.879883</td>\n",
       "      <td>...</td>\n",
       "      <td>1.496094</td>\n",
       "      <td>2.849609</td>\n",
       "      <td>-2.357422</td>\n",
       "      <td>1.930664</td>\n",
       "      <td>-1.068359</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>-0.424805</td>\n",
       "      <td>1.227539</td>\n",
       "      <td>-0.494873</td>\n",
       "      <td>-0.307861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>20200102</td>\n",
       "      <td>1.715820</td>\n",
       "      <td>-0.078430</td>\n",
       "      <td>-0.299072</td>\n",
       "      <td>-1.318359</td>\n",
       "      <td>-1.058594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.223633</td>\n",
       "      <td>-0.311523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.913086</td>\n",
       "      <td>-1.262695</td>\n",
       "      <td>-1.708984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.511719</td>\n",
       "      <td>0.481689</td>\n",
       "      <td>0.795898</td>\n",
       "      <td>2.093750</td>\n",
       "      <td>0.141724</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>20200102</td>\n",
       "      <td>1.521484</td>\n",
       "      <td>0.557617</td>\n",
       "      <td>-0.724121</td>\n",
       "      <td>0.151978</td>\n",
       "      <td>0.282715</td>\n",
       "      <td>0.235352</td>\n",
       "      <td>0.616699</td>\n",
       "      <td>-0.702148</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.884766</td>\n",
       "      <td>-0.693359</td>\n",
       "      <td>0.173950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.334961</td>\n",
       "      <td>-0.145386</td>\n",
       "      <td>-0.639160</td>\n",
       "      <td>2.347656</td>\n",
       "      <td>-0.695312</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>20200102</td>\n",
       "      <td>1.281250</td>\n",
       "      <td>0.491699</td>\n",
       "      <td>-0.707031</td>\n",
       "      <td>-0.217407</td>\n",
       "      <td>-0.359863</td>\n",
       "      <td>-0.239624</td>\n",
       "      <td>-0.324951</td>\n",
       "      <td>-0.658203</td>\n",
       "      <td>...</td>\n",
       "      <td>1.476562</td>\n",
       "      <td>0.088257</td>\n",
       "      <td>0.987793</td>\n",
       "      <td>1.947266</td>\n",
       "      <td>-0.292969</td>\n",
       "      <td>-0.246582</td>\n",
       "      <td>-1.270508</td>\n",
       "      <td>0.121155</td>\n",
       "      <td>-0.814453</td>\n",
       "      <td>-0.664551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26791</th>\n",
       "      <td>688778</td>\n",
       "      <td>20200109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26792</th>\n",
       "      <td>688779</td>\n",
       "      <td>20200109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26793</th>\n",
       "      <td>688793</td>\n",
       "      <td>20200109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26794</th>\n",
       "      <td>688799</td>\n",
       "      <td>20200109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26795</th>\n",
       "      <td>688981</td>\n",
       "      <td>20200109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26796 rows × 4323 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index      date  20211217_10_rank  20211217_12_rank  20211217_1_abs  \\\n",
       "0           1  20200102          0.031403          1.644531        1.876953   \n",
       "1           2  20200102         -0.103943          1.631836        1.875000   \n",
       "2           4  20200102          1.715820         -0.078430       -0.299072   \n",
       "3           5  20200102          1.521484          0.557617       -0.724121   \n",
       "4           6  20200102          1.281250          0.491699       -0.707031   \n",
       "...       ...       ...               ...               ...             ...   \n",
       "26791  688778  20200109               NaN               NaN             NaN   \n",
       "26792  688779  20200109               NaN               NaN             NaN   \n",
       "26793  688793  20200109               NaN               NaN             NaN   \n",
       "26794  688799  20200109               NaN               NaN             NaN   \n",
       "26795  688981  20200109               NaN               NaN             NaN   \n",
       "\n",
       "       20211217_1_abs_regwin=1_rank_abs  20211217_1_abs_regwin=20_rank_abs  \\\n",
       "0                              1.681641                           1.674805   \n",
       "1                              1.670898                           1.126953   \n",
       "2                             -1.318359                          -1.058594   \n",
       "3                              0.151978                           0.282715   \n",
       "4                             -0.217407                          -0.359863   \n",
       "...                                 ...                                ...   \n",
       "26791                               NaN                                NaN   \n",
       "26792                               NaN                                NaN   \n",
       "26793                               NaN                                NaN   \n",
       "26794                               NaN                                NaN   \n",
       "26795                               NaN                                NaN   \n",
       "\n",
       "       20211217_1_abs_regwin=40_rank_abs  20211217_1_abs_regwin=5_rank_abs  \\\n",
       "0                               1.673828                          1.690430   \n",
       "1                               1.338867                          1.666016   \n",
       "2                                    NaN                         -1.223633   \n",
       "3                               0.235352                          0.616699   \n",
       "4                              -0.239624                         -0.324951   \n",
       "...                                  ...                               ...   \n",
       "26791                                NaN                               NaN   \n",
       "26792                                NaN                               NaN   \n",
       "26793                                NaN                               NaN   \n",
       "26794                                NaN                               NaN   \n",
       "26795                                NaN                               NaN   \n",
       "\n",
       "       20211217_1_ori_regwin=10_abs  ...  market_leverage  \\\n",
       "0                          1.885742  ...              NaN   \n",
       "1                          1.879883  ...         1.496094   \n",
       "2                         -0.311523  ...        -0.913086   \n",
       "3                         -0.702148  ...        -0.884766   \n",
       "4                         -0.658203  ...         1.476562   \n",
       "...                             ...  ...              ...   \n",
       "26791                           NaN  ...              NaN   \n",
       "26792                           NaN  ...              NaN   \n",
       "26793                           NaN  ...              NaN   \n",
       "26794                           NaN  ...              NaN   \n",
       "26795                           NaN  ...              NaN   \n",
       "\n",
       "       natural_log_of_market_cap  non_linear_size  \\\n",
       "0                       2.830078        -2.357422   \n",
       "1                       2.849609        -2.357422   \n",
       "2                      -1.262695        -1.708984   \n",
       "3                      -0.693359         0.173950   \n",
       "4                       0.088257         0.987793   \n",
       "...                          ...              ...   \n",
       "26791                        NaN              NaN   \n",
       "26792                        NaN              NaN   \n",
       "26793                        NaN              NaN   \n",
       "26794                        NaN              NaN   \n",
       "26795                        NaN              NaN   \n",
       "\n",
       "       predicted_earnings_to_price_ratio  raw_beta  relative_strength  \\\n",
       "0                               1.259766 -0.500000           1.269531   \n",
       "1                               1.930664 -1.068359           0.656250   \n",
       "2                                    NaN -0.511719           0.481689   \n",
       "3                                    NaN  0.334961          -0.145386   \n",
       "4                               1.947266 -0.292969          -0.246582   \n",
       "...                                  ...       ...                ...   \n",
       "26791                                NaN       NaN                NaN   \n",
       "26792                                NaN       NaN                NaN   \n",
       "26793                                NaN       NaN                NaN   \n",
       "26794                                NaN       NaN                NaN   \n",
       "26795                                NaN       NaN                NaN   \n",
       "\n",
       "       residual_volatility  sales_growth  share_turnover_monthly  \\\n",
       "0                -0.447510           NaN               -1.295898   \n",
       "1                -0.424805      1.227539               -0.494873   \n",
       "2                 0.795898      2.093750                0.141724   \n",
       "3                -0.639160      2.347656               -0.695312   \n",
       "4                -1.270508      0.121155               -0.814453   \n",
       "...                    ...           ...                     ...   \n",
       "26791                  NaN           NaN                     NaN   \n",
       "26792                  NaN           NaN                     NaN   \n",
       "26793                  NaN           NaN                     NaN   \n",
       "26794                  NaN           NaN                     NaN   \n",
       "26795                  NaN           NaN                     NaN   \n",
       "\n",
       "       short_term_predicted_earnings_growth  \n",
       "0                                 -0.309814  \n",
       "1                                 -0.307861  \n",
       "2                                       NaN  \n",
       "3                                       NaN  \n",
       "4                                 -0.664551  \n",
       "...                                     ...  \n",
       "26791                                   NaN  \n",
       "26792                                   NaN  \n",
       "26793                                   NaN  \n",
       "26794                                   NaN  \n",
       "26795                                   NaN  \n",
       "\n",
       "[26796 rows x 4323 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd=pd.read_feather('/data/disk4/output_stocks/jmchen/factors/ML/combine20.feather')\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.set_index('date').rename(columns={'index','code'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
